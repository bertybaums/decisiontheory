<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="Chapter 10 Conditional Probabilities and Likelihoods | Decision Theory" />
<meta property="og:type" content="book" />

<meta property="og:description" content="This is a draft of a decision theory book written in BookDown" />
<meta name="github-repo" content="bertybaums/decisiontheory" />

<meta name="author" content="Bert Baumgaertner" />

<meta name="date" content="2022-10-24" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>

<meta name="description" content="This is a draft of a decision theory book written in BookDown">

<title>Chapter 10 Conditional Probabilities and Likelihoods | Decision Theory</title>

<link href="libs/tufte-css-2015.12.29/tufte-fonts.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-background.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte-italics.css" rel="stylesheet" />
<link href="libs/tufte-css-2015.12.29/tufte.css" rel="stylesheet" />





<link rel="stylesheet" href="toc-vip.css" type="text/css" />
<link rel="stylesheet" href="custom.css" type="text/css" />

</head>

<body>


<div style="display: none;">
$$
  \definecolor{bookorange}{RGB}{255,140,0}
  \definecolor{bookblue}{RGB}{0,92,169}
  \definecolor{bookpurple}{RGB}{148,0,211}
$$
</div>

<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li class="has-sub"><a href="index.html#preface-and-prerequisites">Preface and Prerequisites</a><ul>
<li><a href="index.html#what-is-a-conceptual-introduction"><span class="toc-section-number">0.1</span> What is a conceptual introduction?</a></li>
<li><a href="index.html#how-to-read-a-table-or-matrix"><span class="toc-section-number">0.2</span> How to read a table or matrix</a></li>
<li><a href="index.html#the-very-basic-math"><span class="toc-section-number">0.3</span> The Very Basic Math</a></li>
<li><a href="index.html#inspiration-and-acknowledgments"><span class="toc-section-number">0.4</span> Inspiration and Acknowledgments</a></li>
</ul></li>
<li class="has-sub"><a href="intro.html#intro"><span class="toc-section-number">1</span> Introduction</a><ul>
<li><a href="intro.html#some-basic-conceptual-ingredients"><span class="toc-section-number">1.1</span> Some Basic Conceptual Ingredients</a></li>
<li><a href="intro.html#rationality---the-descriptive-and-normative"><span class="toc-section-number">1.2</span> Rationality - the Descriptive and Normative</a></li>
<li><a href="intro.html#uncertainty"><span class="toc-section-number">1.3</span> Uncertainty</a></li>
<li><a href="intro.html#practical-and-theoretical-problems"><span class="toc-section-number">1.4</span> Practical and Theoretical Problems</a></li>
<li><a href="intro.html#summary"><span class="toc-section-number">1.5</span> Summary</a></li>
<li><a href="intro.html#exercises">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="ranking.html#ranking"><span class="toc-section-number">2</span> Ranking</a><ul>
<li><a href="ranking.html#maximin-and-maximax"><span class="toc-section-number">2.1</span> Maximin and Maximax</a></li>
<li><a href="ranking.html#the-dominance-principle"><span class="toc-section-number">2.2</span> The Dominance Principle</a></li>
<li><a href="ranking.html#more-than-two-options-and-two-states"><span class="toc-section-number">2.3</span> More than two options and two states</a></li>
<li><a href="ranking.html#non-unique-recommendations"><span class="toc-section-number">2.4</span> Non-Unique Recommendations</a></li>
<li><a href="ranking.html#independence-of-options-and-states"><span class="toc-section-number">2.5</span> Independence of Options and States</a></li>
<li><a href="ranking.html#exercises-1">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="transitivity-and-completeness.html#transitivity-and-completeness"><span class="toc-section-number">3</span> Transitivity and Completeness</a><ul>
<li><a href="transitivity-and-completeness.html#notation"><span class="toc-section-number">3.1</span> Notation</a></li>
<li><a href="transitivity-and-completeness.html#money-pump-arguments-for-axioms"><span class="toc-section-number">3.2</span> Money Pump Arguments for Axioms</a></li>
<li><a href="transitivity-and-completeness.html#arguments-for-transitivity"><span class="toc-section-number">3.3</span> Arguments for Transitivity</a></li>
<li><a href="transitivity-and-completeness.html#arguments-for-completeness"><span class="toc-section-number">3.4</span> Arguments for Completeness</a></li>
<li><a href="transitivity-and-completeness.html#social-choice"><span class="toc-section-number">3.5</span> Social Choice</a></li>
<li><a href="transitivity-and-completeness.html#limitations-and-key-take-aways"><span class="toc-section-number">3.6</span> Limitations and Key Take Aways</a></li>
<li><a href="transitivity-and-completeness.html#exercises-2">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="utilities.html#utilities"><span class="toc-section-number">4</span> Utilities</a><ul>
<li><a href="utilities.html#creating-an-interval-scale"><span class="toc-section-number">4.1</span> Creating an Interval Scale</a></li>
<li><a href="utilities.html#what-do-the-numbers-mean"><span class="toc-section-number">4.2</span> What do the numbers mean?</a></li>
<li><a href="utilities.html#applications-and-challenges"><span class="toc-section-number">4.3</span> Applications and Challenges</a></li>
<li><a href="utilities.html#more-challenges-and-final-remarks"><span class="toc-section-number">4.4</span> More Challenges and Final Remarks</a></li>
<li><a href="utilities.html#exercises-3">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="expected-utilities.html#expected-utilities"><span class="toc-section-number">5</span> Expected Utilities</a><ul>
<li><a href="expected-utilities.html#expected-utility-by-example"><span class="toc-section-number">5.1</span> Expected Utility by Example</a></li>
<li><a href="expected-utilities.html#meu-maximize-expected-utility-strategy"><span class="toc-section-number">5.2</span> (MEU) Maximize Expected Utility Strategy</a></li>
<li><a href="expected-utilities.html#application-combining-meu-and-the-multi-attribute-approach"><span class="toc-section-number">5.3</span> Application: Combining MEU and the Multi-Attribute Approach</a></li>
<li><a href="expected-utilities.html#pascals-wager"><span class="toc-section-number">5.4</span> Pascal’s Wager</a></li>
<li><a href="expected-utilities.html#key-take-aways"><span class="toc-section-number">5.5</span> Key Take Aways</a></li>
<li><a href="expected-utilities.html#exercises-4">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="arguments-about-meu.html#arguments-about-meu"><span class="toc-section-number">6</span> Arguments about MEU</a><ul>
<li><a href="arguments-about-meu.html#the-domain-of-meu"><span class="toc-section-number">6.1</span> The Domain of MEU</a></li>
<li><a href="arguments-about-meu.html#long-run-arguments-for-meu"><span class="toc-section-number">6.2</span> Long Run Arguments for MEU</a></li>
<li><a href="arguments-about-meu.html#two-kinds-of-arguments-against-meu"><span class="toc-section-number">6.3</span> Two Kinds of Arguments Against MEU</a></li>
<li><a href="arguments-about-meu.html#arguments-against-normative-meu"><span class="toc-section-number">6.4</span> Arguments Against Normative MEU</a></li>
<li><a href="arguments-about-meu.html#arguments-against-descriptive-meu"><span class="toc-section-number">6.5</span> Arguments Against Descriptive MEU</a></li>
<li><a href="arguments-about-meu.html#summary-1"><span class="toc-section-number">6.6</span> Summary</a></li>
<li><a href="arguments-about-meu.html#exercises-5"><span class="toc-section-number">6.7</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="intervention.html#intervention"><span class="toc-section-number">7</span> Intervention</a><ul>
<li><a href="intervention.html#causal-models"><span class="toc-section-number">7.1</span> Causal Models</a></li>
<li><a href="intervention.html#common-causes"><span class="toc-section-number">7.2</span> Common Causes</a></li>
<li><a href="intervention.html#application-to-newcomb-like-problems"><span class="toc-section-number">7.3</span> Application to Newcomb-like Problems</a></li>
<li><a href="intervention.html#the-locus-of-choice-and-types-of-decision-theories"><span class="toc-section-number">7.4</span> The Locus of Choice and Types of Decision Theories</a></li>
<li><a href="intervention.html#exercises-6">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="odds-probabilities-and-actions.html#odds-probabilities-and-actions"><span class="toc-section-number">8</span> Odds, Probabilities and Actions</a><ul>
<li><a href="odds-probabilities-and-actions.html#odds-and-fair-betting-rates"><span class="toc-section-number">8.1</span> Odds and Fair Betting Rates</a></li>
<li><a href="odds-probabilities-and-actions.html#advantageous-bets"><span class="toc-section-number">8.2</span> Advantageous Bets</a></li>
<li><a href="odds-probabilities-and-actions.html#the-axioms-of-probability-and-dutchbooks"><span class="toc-section-number">8.3</span> The Axioms of Probability and Dutchbooks</a></li>
<li><a href="odds-probabilities-and-actions.html#application"><span class="toc-section-number">8.4</span> Application</a></li>
<li><a href="odds-probabilities-and-actions.html#exercises-7">Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="probabilities-and-logic.html#probabilities-and-logic"><span class="toc-section-number">9</span> Probabilities and Logic</a><ul>
<li><a href="probabilities-and-logic.html#measures"><span class="toc-section-number">9.1</span> Measures</a></li>
<li><a href="probabilities-and-logic.html#normalized-measures"><span class="toc-section-number">9.2</span> Normalized Measures</a></li>
<li><a href="probabilities-and-logic.html#possibilities-and-truth-tables"><span class="toc-section-number">9.3</span> Possibilities and Truth Tables</a></li>
<li><a href="probabilities-and-logic.html#independence"><span class="toc-section-number">9.4</span> Independence</a></li>
<li><a href="probabilities-and-logic.html#summary-2"><span class="toc-section-number">9.5</span> Summary</a></li>
<li><a href="probabilities-and-logic.html#exercises-8"><span class="toc-section-number">9.6</span> Exercises</a></li>
</ul></li>
<li class="has-sub"><a href="conditional-probabilities-and-likelihoods.html#conditional-probabilities-and-likelihoods"><span class="toc-section-number">10</span> Conditional Probabilities and Likelihoods</a><ul>
<li><a href="conditional-probabilities-and-likelihoods.html#calculating-conditional-probability"><span class="toc-section-number">10.1</span> Calculating Conditional Probability</a></li>
<li><a href="conditional-probabilities-and-likelihoods.html#application-monty-hall-problem"><span class="toc-section-number">10.2</span> Application: Monty Hall Problem</a></li>
<li><a href="conditional-probabilities-and-likelihoods.html#likelihoods"><span class="toc-section-number">10.3</span> Likelihoods</a></li>
<li><a href="conditional-probabilities-and-likelihoods.html#application-the-taxi-cab-problem"><span class="toc-section-number">10.4</span> Application: The Taxi Cab Problem</a></li>
</ul></li>
<li><a href="base-rates-priors-and-bayes-rule.html#base-rates-priors-and-bayes-rule"><span class="toc-section-number">11</span> Base Rates, Priors, and Bayes Rule</a></li>
<li><a href="learning-and-motivated-reasoning.html#learning-and-motivated-reasoning"><span class="toc-section-number">12</span> Learning and Motivated Reasoning</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="conditional-probabilities-and-likelihoods" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Conditional Probabilities and Likelihoods</h1>
<p>Assuming you have a driver’s licence and the roads are relatively clear, the chances of crashing your car are pretty low. But if you’re drink, the chances of crash are much higher. Probabilities change depending on the conditions.</p>
<p>We already have notation for symbolizing this idea. We use <span class="math inline">\(P(A | B)\)</span> to represent the probability that <span class="math inline">\(A\)</span> is true <em>given</em> that <span class="math inline">\(B\)</span> is true. For example, to say the probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is 30%, we write:
<span class="math display">\[ P(A | B) = .3 \]</span>
When we condition probabilities in this way, we call them <em>conditional probabilities</em>. Conditional probabilities play a central role in the remaining material, so let’s spend some time learning how to calculate them.</p>
<div id="calculating-conditional-probability" class="section level2">
<h2><span class="header-section-number">10.1</span> Calculating Conditional Probability</h2>
<p><label for="tufte-mn-6" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-6" class="margin-toggle"><span class="marginnote"><span style="display: block;">Most of this presentation is a light edit of Weisberg’s introduction to calculating conditional probability (6.1).</span></span></p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="decisiontheory_files/figure-html/unnamed-chunk-24-1.png" alt="Conditional probability in a fair die roll" width="672"  /><img src="decisiontheory_files/figure-html/unnamed-chunk-24-2.png" alt="Conditional probability in a fair die roll" width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.1: Conditional probability in a fair die roll<!--</p>-->
<!--</div>--></span>
</p>
<p>Suppose I roll a fair, six-sided die behind a screen. You can’t see the result, but I tell you it’s an even number. What’s the probability it’s also a “high” number: either a <span class="math inline">\(4\)</span>, <span class="math inline">\(5\)</span>, or <span class="math inline">\(6\)</span>?</p>
<p>Maybe you figured the correct answer: <span class="math inline">\(2/3\)</span>. But why is that correct? Because, out of the three even numbers (<span class="math inline">\(2\)</span>, <span class="math inline">\(4\)</span>, and <span class="math inline">\(6\)</span>), two of them are high (<span class="math inline">\(4\)</span> and <span class="math inline">\(6\)</span>). And since the die is fair, we expect it to land on a high number <span class="math inline">\(2/3\)</span> of the times it lands on an even number.</p>
<p>This hints at a formula for <span class="math inline">\(P(A | B)\)</span>.</p>
<dl>
<dt>Conditional Probability</dt>
<dd><p><span class="math display">\[ P(A | B) = \frac{P(A \wedge B)}{P(B)}. \]</span></p>
</dd>
</dl>
<p>In the die-roll example, we considered how many of the <span class="math inline">\(B\)</span> possibilities were also <span class="math inline">\(A\)</span> possibilities. Which means we divided <span class="math inline">\(P(A \wedge B)\)</span> by <span class="math inline">\(P(B)\)</span>.</p>
<p>In fact, this formula is our official definition for the concept of conditional probability. When we write the sequence of symbols <span class="math inline">\(P(A | B)\)</span>, it’s really just shorthand for the fraction <span class="math inline">\(P(A \wedge B) / P(B)\)</span>.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:condprob"></span>
<img src="decisiontheory_files/figure-html/condprob-1.png" alt="Conditional probability is the size of the $A \wedge B$ region compared to the entire $B$ region." width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.2: Conditional probability is the size of the <span class="math inline">\(A \wedge B\)</span> region compared to the entire <span class="math inline">\(B\)</span> region.<!--</p>-->
<!--</div>--></span>
</p>
<p>In terms of an Euler diagram (Figure <a href="conditional-probabilities-and-likelihoods.html#fig:condprob">10.2</a>), the definition of conditional probability compares the size of the purple <span class="math inline">\(A \wedge B\)</span> region to the size of the whole <span class="math inline">\(B\)</span> region, purple and blue together. If you don’t mind getting a little colourful with your algebra:
<span class="math display">\[
  P(A | B) = \frac{\color{bookpurple}{\blacksquare}}{\color{bookpurple}{\blacksquare} + \color{bookblue}{\blacksquare}}.
\]</span>
So the definition works because, informally speaking, <span class="math inline">\(P(A \wedge B)/P(B)\)</span> is the proportion of the <span class="math inline">\(B\)</span> outcomes that are also <span class="math inline">\(A\)</span> outcomes.</p>
<p><span class="newthought">Dividing</span> by zero is a common pitfall with conditional probability. Notice how the definition of <span class="math inline">\(P(A | B)\)</span> depends on <span class="math inline">\(P(B)\)</span> being larger than zero. If <span class="math inline">\(P(B) = 0\)</span>, then the formula
<label for="tufte-mn-7" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-7" class="margin-toggle"><span class="marginnote"><span style="display: block;">The comedian Steven Wright once quipped that “black holes are where God divided by zero.”</span></span>
<span class="math display">\[ P(A | B) = \frac{P(A \wedge B)}{P(B)} \]</span>
doesn’t even make any sense. There is no number that results from the division on the right hand side.<label for="tufte-sn-88" class="margin-toggle sidenote-number">88</label><input type="checkbox" id="tufte-sn-88" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">88</span> There are alternative mathematical systems of probability, where conditional probability is defined differently to avoid this problem. But we’ll stick to the standard system. In this system, there’s just no such thing as “the probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span>” when <span class="math inline">\(B\)</span> has zero probability.</span></p>
<p>In such cases we say that <span class="math inline">\(P(A | B)\)</span> is <em>undefined</em>. It’s not zero, or some special number. It just isn’t a number.</p>
</div>
<div id="application-monty-hall-problem" class="section level2">
<h2><span class="header-section-number">10.2</span> Application: Monty Hall Problem</h2>
<p>Here we’ll show how the concept of conditional probability allows us to solve the Monty Hall problem in the introduction. Here’s the problem description:</p>
<blockquote>
<p>On the show there are three doors (A, B, and C), one of which with a prize behind it. You get to pick one of the doors. Let’s say you pick A. The host now opens one of the other two doors that you did not pick. But of course, the host doesn’t want to give away the game, so the door they open will be empty. After opening one of the two doors (B or C) the host asks, do you want to switch your choice or stick with your current choice of A?</p>
</blockquote>
<p>The intuitive answer, one that many mathematicians and statisticians gave at the time, is that you should be indifferent between switching and staying with your choice of door A. Why? Because, the (incorrect) reasoning goes, there’s two doors (A and whichever one the host didn’t open) and an even chance between them of where the prize is. Notice that this question is about an <em>unconditional</em> probability.</p>
<p>The problem with this reasoning is that it ignores the events that proceeded. The reasoning would be apt if the game show had you picking between just two doors from the very start, and just because the host reveals what’s behind the door you didn’t pick, they ask you if you want to change your mind. But that’s not what’s going on the in the Monty Hall problem. The real question is: should you switch your choice from A <em>given that the host opened a non-prize door after your initial choice</em>? Notice that this question is about a <em>conditional</em> probability.<label for="tufte-sn-89" class="margin-toggle sidenote-number">89</label><input type="checkbox" id="tufte-sn-89" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">89</span> What the host does is <em>not independent</em> of your initial guess.</span></p>
<p>If the prize is behind door A (the door you initially picked), then the host has a choice between opening up door B or door C. But if the prize isn’t behind door A, then the host is constrained. If the prize is behind door B, then the host will open C. If the prize is behind C, then the host will open B. What we’re reasoning about here are <em>paths</em> of possible events.</p>
<p>The first kind of event is random, it’s just about the location of the prize behind one of the three doors. So the probability of the prize being behind door A is 1/3, and similarly for doors B and C. In other words, your initial guess of door A has a 1/3 chance of being right.</p>
<p>A diagram can help (see Stage 1). Each arrow is a branch and has a probability associated with it.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:unnamed-chunk-26"></span>
<img src="decisiontheory_files/figure-html/unnamed-chunk-26-1.png" alt="Stage 1 of tree diagram" width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.3: Stage 1 of tree diagram<!--</p>-->
<!--</div>--></span>
</p>
<p>The second kind of event is the host’s reveal of a non-prize door, which is <em>not</em> random <em>if</em> your choice of door A is <em>incorrect</em>, and is random if your choice of A is correct. Let’s build on the diagram above, showing what the host’s options are (see Stage 2). When the prize is behind door A (your guess) the host has two options, so we have two branches, each with a probability of 1/2. When the prize is behind door B, there’s only one thing the host can do (so that branch has a probability of 1, which we don’t bother labeling). Same thing when the prize is behind door C.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:unnamed-chunk-27"></span>
<img src="decisiontheory_files/figure-html/unnamed-chunk-27-1.png" alt="Stage 2 of tree diagram" width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.4: Stage 2 of tree diagram<!--</p>-->
<!--</div>--></span>
</p>
<p>Now imagine you could play this game over and over again (thousands if you like!), always making your first pick door A.<label for="tufte-sn-90" class="margin-toggle sidenote-number">90</label><input type="checkbox" id="tufte-sn-90" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">90</span> You can try it for yourself <a href="http://www.rossmanchance.com/applets/2021/montyhall/Monty.html">HERE.</a></span> One third of the time the prize will be behind door A, and of those times, half of them the host opens door B and the other half door C. So that means that the top path where the prize is behind door A and the host opens door B will happen 1/6th of the time. Similarly for the path where the prize is behind door A and the host opens C. For the other two possibilities where the prize is behind door B or door C, the host only has one option, so each of those paths will happen 1/3rd of the time. Here’s our completed diagram with the probabilities at the end of the paths.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-28"></span>
<p class="caption marginnote shownote">
Figure 10.5: Stage 3 of tree diagram
</p>
<img src="decisiontheory_files/figure-html/unnamed-chunk-28-1.png" alt="Stage 3 of tree diagram" width="672"  />
</div>
<p>Of course you don’t actually know which path you are on, but here’s what you do know: When the host opens door C, you are more likely to be on the middle path (1/3) than you are on the path above it (1/6). On the middle path the prize is behind door B, while in the path above the prize is behind door A. Since the middle path is the more likely one you’re on, you should switch your choice from A to B! The same reasoning applies if the host had opened door B instead - then you should switch your choice to C.</p>
<p>Notice that the probabilities at the end of the paths have to taken into account all the branches that lead to that end point. So the probability of being on the branch that goes from A to Open B is <em>conditional</em> on the previous branch that goes to A.</p>
<p>Here’s another example.<label for="tufte-sn-91" class="margin-toggle sidenote-number">91</label><input type="checkbox" id="tufte-sn-91" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">91</span> Thanks to Weisberg’s <em>Odds and Ends</em>.</span></p>
<p>Suppose there are two urns of coloured marbles.</p>
<ul>
<li>Urn X contains 3 black marbles, 1 white.</li>
<li>Urn Y contains 1 black marble, 3 white.</li>
</ul>
<p>We flip a fair coin to decide which urn to draw from, heads for Urn X and tails for Urn Y. Then we draw one marble at random. The following tree diagram shows us the possible sequences of events.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:unnamed-chunk-29"></span>
<img src="decisiontheory_files/figure-html/unnamed-chunk-29-1.png" alt="Coin Flip and Urn Draws" width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.6: Coin Flip and Urn Draws<!--</p>-->
<!--</div>--></span>
</p>
<p>The probability of drawing a black marble on the top path is <span class="math inline">\(3/4\)</span> because we are assuming the coin landed heads, and thus we’re drawing from Urn X. If the coin lands tails instead, and we draw from Urn Y, then the chance of a black marble is instead <span class="math inline">\(1/4\)</span>. So these quantities are conditional probabilities:
<span class="math display">\[
  \begin{aligned}
    Pr(B | H) &amp;= 3/4,\\
    Pr(B | T) &amp;= 1/4.
  \end{aligned}
\]</span>
Notice, though, the first branch in a tree diagram is different. In the <span class="math inline">\(H\)</span>-vs.-<span class="math inline">\(T\)</span> branch, the probabilities are <em>un</em>conditional, since there are no previous branches for them to be conditional on.</p>
</div>
<div id="likelihoods" class="section level2">
<h2><span class="header-section-number">10.3</span> Likelihoods</h2>
<p><span class="newthought">Order matters</span> when it comes to conditional probabilities. For example, given that someone is a university student (S), the probability that they are below 40 years old (F) is relatively high. However, given that someone is below 40 years old, the probability that they are a university student is low. In symbols, <span class="math inline">\(Pr(S|F)\neq Pr(F|S)\)</span>.<label for="tufte-sn-92" class="margin-toggle sidenote-number">92</label><input type="checkbox" id="tufte-sn-92" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">92</span> There are some cases where the two are the same, but that’s rare.</span></p>
<p><span class="newthought">Here’s another example.</span><label for="tufte-sn-93" class="margin-toggle sidenote-number">93</label><input type="checkbox" id="tufte-sn-93" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">93</span> Borrowed from Weisberg’s <em>Odds and Ends</em>, chapter 6.</span> Suppose a university has 10,000 students. Each is studying under one of four broad headings: Humanities, Social Sciences, STEM, or Professional. Under each of these categories, the number of students with an average grade of A, B, C, or D is listed in the following table. What is the probability a randomly selected student will have an A average, given that they are studying either Humanities or Social Sciences?</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Humanities</th>
<th align="center">Social Sciences</th>
<th align="center">STEM</th>
<th align="center">Professional</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">A</td>
<td align="center">200</td>
<td align="center">600</td>
<td align="center">400</td>
<td align="center">900</td>
</tr>
<tr class="even">
<td align="left">B</td>
<td align="center">500</td>
<td align="center">800</td>
<td align="center">1600</td>
<td align="center">900</td>
</tr>
<tr class="odd">
<td align="left">C</td>
<td align="center">250</td>
<td align="center">400</td>
<td align="center">1500</td>
<td align="center">750</td>
</tr>
<tr class="even">
<td align="left">D</td>
<td align="center">50</td>
<td align="center">200</td>
<td align="center">500</td>
<td align="center">450</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
  \begin{aligned}
    Pr(A ~ | ~(H \vee S)~) &amp;= \frac{Pr(A \wedge (H \vee S))}{Pr(H \vee S)}\\
                           &amp;= \frac{800/10,000}{3,000/10,000}\\
                           &amp;= 4/15.
  \end{aligned}
\]</span>
What about the reverse probability, that a student is studying either Humanities or Social Sciences given that they have an A average?
<span class="math display">\[
  \begin{aligned}
    Pr((H \vee S) ~ | ~ A) &amp;= \frac{Pr((H \vee S) \wedge A)}{Pr(A)}\\
                           &amp;= \frac{800/10,000}{2,100/10,000}\\
                           &amp;= 8/21.
  \end{aligned}
\]</span>
Notice how we get a different number now.</p>
<p><span class="newthought">When we reason about hypotheses</span> the order is particularly important to get right, so much so that there is a particular term that scientists will use to indicate the direction of the conditional probability. This is called a <em>likelihood</em>. To understand it, we need to make an important distinction between a hypothesis (or theory) and evidence (or data or observation).</p>
<p><span class="newthought">For example,</span> suppose a company BestShoes claims that 96% of their shoes outlast their competitor’s average shoe mileage of 300 miles. Furthermore, lets say that a running magazine decides to test BestShoes’ claim by sending out 2,400 shoes from BestShoes to the magazine’s subscribers. The magazine finds that 133 of the shoes did not make it past 300 miles before falling apart. The <em>hypothesis</em> is the claim that each shoe has a 0.96 probability of making it past 300 miles. The <em>evidence</em> is the claim that 2,277 of 2,400 shoes made it past 300 miles.<label for="tufte-sn-94" class="margin-toggle sidenote-number">94</label><input type="checkbox" id="tufte-sn-94" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">94</span> Later we’ll see how to go about quantifying the degree of support (or lack there of) that the evidence gives to the hypothesis.</span></p>
<p>It will be convenient for us to use the letter <em>H</em> when talking about some hypothesis and <em>E</em> for some statement of evidence.</p>
<p>In general, a <em>hypothesis</em> is a statement about what is (or will be) the case. Our beliefs about the world are similar to scientific hypotheses (though perhaps not as technically stated) in the following sense:</p>
<ul>
<li><ol style="list-style-type: lower-roman">
<li>there is the content of our belief, which is the statement (e.g. that BestShoes outlast their competitors), and then there is</li>
</ol></li>
<li><ol start="2" style="list-style-type: lower-roman">
<li>the degree of confidence that the statement is true, which we previously learned to (indirectly) measure with the concept of fair bets. We have been representing this degree of confidence as a (personal) probability that a proposition is true, e.g. <span class="math inline">\(Pr(H)=0.75\)</span> means a person is willing to take 3:1 odds in favor that <span class="math inline">\(H\)</span> is true.</li>
</ol></li>
</ul>
<p>The idea of <em>confidence</em> as a degree of belief is not to be confused with <em>reasons</em> for that level of confidence. <em>Evidence</em> is a kind of reason that can be used to <em>support</em> a hypothesis. Evidence gives us reasons for believing that a hypothesis is true: more evidence should make us more confident.</p>
<p>The relationship between hypothesis and evidence can be tricky, in part because there are two different directions that are not equivalent. Recall that most of the time <span class="math inline">\(P(A|B)\neq P(B|A)\)</span>. That same lesson holds for hypothesis and evidence: <span class="math inline">\(P(H|E)\neq P(E|H)\)</span>.<label for="tufte-sn-95" class="margin-toggle sidenote-number">95</label><input type="checkbox" id="tufte-sn-95" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">95</span> In words: the probability that a hypothesis is true <em>given</em> that the evidence is true <em>is not equal to</em> the probability that the evidence is true given that the hypothesis is true.</span></p>
<p>Consider the following two conditional statements:</p>
<ol style="list-style-type: decimal">
<li>If we have the body of evidence <span class="math inline">\(E\)</span>, how probable is it that hypothesis <span class="math inline">\(H\)</span> is true?</li>
<li>If hypothesis <span class="math inline">\(H\)</span> is true, then how likely is it that we would see the body of evidence <span class="math inline">\(E\)</span>?</li>
</ol>
<p>It is a bit of curious fact that a great deal of statistics focuses on developing tools for understanding claim (2) when what we typically want to know is claim (1). For example, in most statistics classes that focus on hypothesis testing, the running intuition goes something like this:</p>
<blockquote>
<p>If the hypothesis we’re testing is true, then how unusual would this result (the evidence) be?</p>
</blockquote>
<p>The core idea behind all the statistical machinery (specifically what’s called <em>frequentist</em> statistics) is that if the evidence would be unusual below some threshold level (called <span class="math inline">\(\alpha\)</span> - “alpha”) <em>under the hypothetical assumption that the hypothesis were true</em>, then that supports or gives us reason to think that the hypothesis is true. If that sounds confusing, it’s because it is. What we’re trying to say is something like, “if the hypothesis were true, then this evidence we’re seeing would be <em>too much of a coincidence</em>”. If that doesn’t help, you’re not alone. Even scientists and statisticians have been found to misunderstand the idea.<label for="tufte-sn-96" class="margin-toggle sidenote-number">96</label><input type="checkbox" id="tufte-sn-96" class="margin-toggle"><span class="sidenote"><span class="sidenote-number">96</span> See <a href="https://jonathanweisberg.org/vip/significance-testing.html#significance-testing" class="uri">https://jonathanweisberg.org/vip/significance-testing.html#significance-testing</a> for an excellent introduction with examples of hypothesis testing.</span></p>
<p>The point we need to make for our purposes is this. When scientists talk about likelihoods, they don’t mean just any kind of probability. What they mean is as follows.</p>
<p><span class="newthought">A likelihood</span> is a conditional probability of seeing some evidence given the assumption that a hypothesis is true, i.e. <span class="math inline">\(P(E|H)\)</span>.</p>
</div>
<div id="application-the-taxi-cab-problem" class="section level2">
<h2><span class="header-section-number">10.4</span> Application: The Taxi Cab Problem</h2>
<p>If there’s anything to take away from this chapter about how conditional probabilities are used in (scientific) reasoning, it’s to make sure you remember that <span class="math inline">\(P(E|H)\neq P(H|E)\)</span>.</p>
<p><span class="newthought">To illustrate</span> this point, consider the following famous taxi cab problem.
<label for="tufte-mn-8" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-8" class="margin-toggle"><span class="marginnote"><span style="display: block;">The experiment was first published in 1971. It was performed by <a href="https://en.wikipedia.org/wiki/Daniel_Kahneman">Daniel Kahneman</a> and <a href="https://en.wikipedia.org/wiki/Amos_Tversky">Amos Tversky</a>. Their work on human reasoning reshaped the field of psychology, and eventually won a Nobel prize in 2002.</span></span></p>
<p>A cab was involved in a hit and run accident at night. Two cab companies, the Green and the Blue, operate in the city. You are given the following data:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(85\%\)</span> of the cabs in the city are Green and <span class="math inline">\(15\%\)</span> are Blue.</li>
<li>A witness identified the cab as Blue. The court tested the reliability of the witness under the same circumstances that existed on the night of the accident and concluded that the witness correctly identified each one of the two colors <span class="math inline">\(80\%\)</span> of the time and failed <span class="math inline">\(20\%\)</span> of the time.</li>
</ol>
<p>What is the probability that the cab involved in the accident was blue rather green?</p>
<p>Most people answer <span class="math inline">\(80\%\)</span>, because the witness is <span class="math inline">\(80\%\)</span> reliable. But the right answer is <span class="math inline">\(12/29\)</span>, or about <span class="math inline">\(41\%\)</span>.</p>
<p>How could the probability be so low when the witness is <span class="math inline">\(80\%\)</span> reliable? The short answer is: because blue cabs are rare. So most of the time, when the witness says a cab is blue, it’s one of the <span class="math inline">\(20\%\)</span> of green cabs they mistakenly identify as blue.</p>
<p>A diagram can help.</p>
<p>
<span class="marginnote shownote">
<!--
<div class="figure">--><span style="display:block;" id="fig:taxigrid"></span>
<img src="decisiontheory_files/figure-html/taxigrid-1.png" alt="The taxicab problem. There are $15$ blue cabs, $85$ green. The dashed region indicates those cabs the witness identifies as &quot;blue.&quot; It includes $80\%$ of the blue cabs ($12$), and only $20\%$ of the green ones ($17$). Yet it includes more green cabs than blue." width="672"  />
<!--
<p class="caption marginnote">-->Figure 10.7: The taxicab problem. There are <span class="math inline">\(15\)</span> blue cabs, <span class="math inline">\(85\)</span> green. The dashed region indicates those cabs the witness identifies as “blue.” It includes <span class="math inline">\(80\%\)</span> of the blue cabs (<span class="math inline">\(12\)</span>), and only <span class="math inline">\(20\%\)</span> of the green ones (<span class="math inline">\(17\)</span>). Yet it includes more green cabs than blue.<!--</p>-->
<!--</div>--></span>
</p>
<p>Imagine there are just <span class="math inline">\(100\)</span> cabs in town, <span class="math inline">\(85\)</span> green and <span class="math inline">\(15\)</span> blue. The dashed blue line represents the cabs the witness identifies as
“blue,” both right or wrong. Because the witness is <span class="math inline">\(80\%\)</span> accurate, that line encompasses <span class="math inline">\(80\%\)</span> of the blue cabs, which is <span class="math inline">\(12\)</span> cabs. But it also encompasses <span class="math inline">\(20\%\)</span> of the green cabs, which is <span class="math inline">\(17\)</span>. That’s a total of <span class="math inline">\(29\)</span> cabs identified as “blue,” only <span class="math inline">\(12\)</span> of which actually are blue.</p>
<p>So out of the <span class="math inline">\(29\)</span> cabs the witness calls “blue,” only <span class="math inline">\(12\)</span> really are blue. The probability a cab really is blue given the witness says so is only <span class="math inline">\(12/29\)</span>, about <span class="math inline">\(41\%\)</span>.</p>
<p>Another way to think about the problem is that there are <em>two</em> pieces of information relevant to whether the cab is blue. The witness says the cab is blue, but also, most cabs are not blue. So there’s evidence for the cab being blue, but also strong evidence against it. The diagram shows us how to balance these two, competing pieces of evidence and come to the correct answer.</p>
<p>What trips people up so much in the taxicab problem? Remember how order matters with conditional probability. In this problem, we’re asked to find <span class="math inline">\(Pr(B | W)\)</span>, the probability the cab is blue given that the witness says it is. That’s not the same as <span class="math inline">\(Pr(W | B)\)</span>, the probability the witness will say the cab is blue if it really is. The problem tells us <span class="math inline">\(Pr(W | B) = 8/10\)</span>, but it doesn’t tell us a number for <span class="math inline">\(Pr(B | W)\)</span>. We have to figure that out.</p>
<p><span class="newthought">It’s not that the two conditional probabilities</span> aren’t related. The taxi cab problem highlights why we should keep the two distinct, but we’d be wrong to think that they aren’t related at all. In fact a very famous theorem called <em>Bayes Theorem</em> shows us a fundamental connection between them. We show that next.</p>
<!-- 
Use two lotteries (or slot machines) as examples.

-->
<!-- Look at Lindley's presentation of exchangeability? (Maybe save this until the learning chapter? Or maybe do one iteration of it here, then return to it again.) -->

</div>
</div>
<p style="text-align: center;">
<a href="probabilities-and-logic.html"><button class="btn btn-default">Previous</button></a>
<a href="base-rates-priors-and-bayes-rule.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>



</body>
</html>
